{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Attention Branch Model with One Class Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install tensorflow==2.12.0\n",
    "! pip3 install soundfile \n",
    "! pip3 install librosa\n",
    "! pip3 install matplotlib "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from functions import *\n",
    "import os \n",
    "import gc\n",
    "from tensorflow.keras.backend import clear_session\n",
    "import hyperparameters as hp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.backend import clear_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "gc.collect()\n",
    "clear_session()\n",
    "directory_path = 'LA/ASVspoof2019_LA_train/flac'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "file_names = []\n",
    "tensors = []\n",
    "\n",
    "directory_path_train = 'LA/ASVspoof2019_LA_train/flac'\n",
    "directory_path_val = 'LA/ASVspoof2019_LA_dev/flac'\n",
    "\n",
    "if os.path.exists('dataset_pickle_dumps/train_dataset_dumped.pkl'):\n",
    "    print(\"LOADING TRAIN DATASET FROM FILE\")\n",
    "    dataset = tf.data.experimental.load('dataset_pickle_dumps/train_dataset_dumped.pkl')\n",
    "    print(\"TRAIN DATASET LOADED FROM FILE\")\n",
    "else:\n",
    "    with open('train_preprocessed.txt', 'r') as file:\n",
    "        train_data = file.readlines()\n",
    "    with open('val_preprocessed.txt', 'r') as file:\n",
    "        val_data = file.readlines()\n",
    "\n",
    "    # Combine data and also keep track of their source\n",
    "    data = [(line, 'train') for line in train_data] + [(line, 'val') for line in val_data]\n",
    "    data = data[:30000]\n",
    "\n",
    "\n",
    "    for line, source in data:\n",
    "        file_name, label = line.split()\n",
    "        file_names.append((file_name, source))\n",
    "        labels.append(int(label))\n",
    "\n",
    "    ctr = 0\n",
    "\n",
    "    for file_name, source in file_names:\n",
    "        ctr += 1\n",
    "        if ctr % 200 == 0:\n",
    "            print(f\"PROGRESS: {ctr}/{len(file_names)}\")\n",
    "        if source == 'train':\n",
    "            file_path = os.path.join(directory_path_train, file_name)\n",
    "        else:\n",
    "            file_path = os.path.join(directory_path_val, file_name)\n",
    "        signal, sample_rate = read_flac_file(file_path)\n",
    "        lfcc_features = extract_lfcc(signal, sample_rate)\n",
    "        tensor = tf.convert_to_tensor(lfcc_features)\n",
    "        tensor = tf.expand_dims(tensor, axis=0)\n",
    "        tensors.append(tensor[0])\n",
    "        \n",
    "    labels = tf.convert_to_tensor(labels)\n",
    "\n",
    "    # pickle dump labels and tensors\n",
    "    import pickle\n",
    "    with open('dataset_pickle_dumps/labels_train.pkl', 'wb') as file:\n",
    "        pickle.dump(labels, file)\n",
    "    with open('dataset_pickle_dumps/tensors_train.pkl', 'wb') as file:\n",
    "        pickle.dump(tensors, file)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((tensors, labels))\n",
    "\n",
    "    dataset = dataset.shuffle(buffer_size=len(tensors)).batch(32)\n",
    "\n",
    "    tf.data.experimental.save(dataset, 'dataset_pickle_dumps/train_dataset_dumped.pkl')\n",
    "\n",
    "    print(\"TRAIN DATASET DUMPED TO FILE\")\n",
    "\n",
    "print(\"NUMBER OF ENTRIES IN TRAINING DATASET: \", len(dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subclassing the Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, ratio=16, **kwargs):\n",
    "        super(SEBlock, self).__init__(**kwargs)\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.filters = input_shape[-1]\n",
    "        self.reshape = layers.Reshape((1, 1, self.filters))\n",
    "        self.conv1 = layers.Conv2D(self.filters // self.ratio, 1, activation='relu',  data_format='channels_last')\n",
    "        self.conv2 = layers.Conv2D(self.filters, 1, activation='sigmoid',  data_format='channels_last')\n",
    "        self.multiply = layers.Multiply()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        se = layers.GlobalAveragePooling2D()(inputs)\n",
    "        se = self.reshape(se)\n",
    "        se = self.conv1(se)\n",
    "        se = self.conv2(se)\n",
    "        return self.multiply([inputs, se])\n",
    "\n",
    "\n",
    "class MBConv1Block (tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel_size=3, filters=16, dropout_rate=0.2):\n",
    "        super(MBConv1Block, self).__init__()\n",
    "        self.dconv = tf.keras.layers.DepthwiseConv2D((kernel_size, kernel_size), padding='same',  data_format='channels_last')\n",
    "        self.swish1 = tf.keras.layers.Activation('swish')\n",
    "        self.batch_norm1 = tf.keras.layers.BatchNormalization()\n",
    "        self.se = SEBlock(ratio = 16)\n",
    "        self.conv = tf.keras.layers.Conv2D(filters, (1, 1), padding='same', activation=tf.keras.activations.sigmoid,  data_format='channels_last')\n",
    "        self.batch_norm1 = tf.keras.layers.BatchNormalization()\n",
    "        self.batch_norm2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "    def call(self, input):\n",
    "        output = self.dconv(input)\n",
    "        output = self.batch_norm1(output)\n",
    "        output = self.swish1(output)\n",
    "        output = self.se(output)\n",
    "        output = self.conv(output)\n",
    "        output = self.batch_norm2(output)\n",
    "        return output\n",
    "    \n",
    "class MBConv6Block (tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel_size=3, filters=16, dropout_rate=0.2):\n",
    "        super(MBConv6Block, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters * 6, (1, 1), padding='same', activation=tf.keras.activations.relu,  data_format='channels_last')\n",
    "        self.batch_norm1 = tf.keras.layers.BatchNormalization()\n",
    "        self.swish1 = tf.keras.layers.Activation('swish')\n",
    "        self.dconv = tf.keras.layers.DepthwiseConv2D((kernel_size, kernel_size), padding='same')\n",
    "        self.swish2 = tf.keras.layers.Activation('swish')\n",
    "        self.batch_norm2 = tf.keras.layers.BatchNormalization()\n",
    "        self.se = SEBlock(ratio = 16)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters, (1, 1), padding='same')\n",
    "        self.batch_norm3 = tf.keras.layers.BatchNormalization()\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.input_conv = tf.keras.layers.Conv2D(self.filters, (1, 1), padding='same')\n",
    "        \n",
    "    def call(self, input):\n",
    "        input_processed = self.input_conv(input)\n",
    "        output = self.conv1(input)\n",
    "        output = self.batch_norm1(output)\n",
    "        output = self.swish1(output)\n",
    "        output = self.dconv(output)\n",
    "        output = self.swish2(output)\n",
    "        output = self.batch_norm2(output)\n",
    "        output = self.se(output)\n",
    "        output = self.conv2(output)\n",
    "        output = self.batch_norm3(output)\n",
    "        output = self.dropout(output)\n",
    "        output = output + input_processed\n",
    "        return output\n",
    "\n",
    "    \n",
    "\n",
    "class BasicBlock (tf.keras.layers.Layer):\n",
    "    def __init__(self,C_in,C_out):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(C_out, (3, 3), padding='same', data_format='channels_last')\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(C_out, (3, 3), padding='same', data_format='channels_last')\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "    \n",
    "    def call(self, input):\n",
    "        input = self.conv1(input)\n",
    "        input = self.bn1(input)\n",
    "        input = self.conv2(input)\n",
    "        input = self.bn2(input)\n",
    "        return input\n",
    "    \n",
    "\n",
    "class AttentionBranch (tf.keras.Model):\n",
    "    def __init__(self,):\n",
    "        super(AttentionBranch, self).__init__()\n",
    "        self.bb1 = BasicBlock(1,2)\n",
    "        self.bb2 = BasicBlock(2,4)\n",
    "        self.bb3 = BasicBlock(4,8)\n",
    "        self.bb4 = BasicBlock(8,16)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(16, (1, 1), padding='same', data_format='channels_last')\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.ab_output_conv = tf.keras.layers.Conv2D(2, (1, 1), padding='same', data_format='channels_last')\n",
    "        self.ab_output_gap = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.ab_output_softmax = tf.keras.layers.Softmax()\n",
    "        self.attention_map_conv = tf.keras.layers.Conv2D(1, (1, 1), padding='same', data_format='channels_last')\n",
    "        self.attention_map_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.attention_map_sigmoid = tf.keras.layers.Activation('sigmoid')\n",
    "    \n",
    "    def call (self, input):\n",
    "        output = self.bb1(input)\n",
    "        output = self.bb2(output)\n",
    "        output = self.bb3(output)\n",
    "        output = self.bb4(output)\n",
    "        output = self.bn1(output)\n",
    "        output = self.conv1(output)\n",
    "        output = self.relu(output)\n",
    "        ab_output =  self.ab_output_conv(output)\n",
    "        ab_output = self.ab_output_gap(ab_output)\n",
    "        ab_output = self.ab_output_softmax(ab_output)\n",
    "        attention_map = self.attention_map_conv(output)\n",
    "        attention_map = self.attention_map_bn(attention_map)\n",
    "        attention_map = self.attention_map_sigmoid(attention_map)\n",
    "        return ab_output, attention_map\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetA0(tf.keras.Model):\n",
    "    def __init__(self, output_dim = 2, dropout=0.2):\n",
    "        super(EfficientNetA0, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, (1, 1), padding='same', data_format='channels_last')\n",
    "        self.mbconv1_1 = MBConv1Block(filters = 32, dropout_rate=dropout)\n",
    "        self.mbconv6_1 = MBConv6Block(filters = 32, dropout_rate=dropout)\n",
    "        self.mbconv6_2 = MBConv6Block(filters = 48, dropout_rate=dropout)\n",
    "        self.mbconv6_3 = MBConv6Block(filters = 48, dropout_rate=dropout)\n",
    "        self.mbconv6_4 = MBConv6Block(filters = 64, dropout_rate=dropout)\n",
    "        self.mbconv6_5 = MBConv6Block(filters = 64, dropout_rate=dropout)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(256, (1, 1), padding='same', data_format='channels_last')\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.global_avg_pool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.dense = tf.keras.layers.Dense(output_dim)\n",
    "         \n",
    "    def call(self, input):\n",
    "        output = self.conv1(input)\n",
    "        output = self.mbconv1_1(output)\n",
    "        output = self.mbconv6_1(output)\n",
    "        output = self.mbconv6_2(output)\n",
    "        output = self.mbconv6_3(output)\n",
    "        output = self.mbconv6_4(output)\n",
    "        output = self.mbconv6_5(output)\n",
    "        output = self.conv2(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.global_avg_pool(output)\n",
    "        output = self.dense(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EABNModel(tf.keras.Model):\n",
    "    def __init__(self, dropout=0.2):\n",
    "        super(EABNModel, self).__init__()\n",
    "        self.efficientnet = EfficientNetA0(dropout=dropout)\n",
    "        self.attention_branch = AttentionBranch()\n",
    "    \n",
    "    def call(self, input):\n",
    "        ab_output, attention_map = self.attention_branch(input)\n",
    "        perception_input = input * attention_map\n",
    "        perception_input = perception_input + input\n",
    "        perception_output = self.efficientnet(perception_input)\n",
    "        return ab_output, perception_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional API Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# def se_block(input_tensor, ratio=16):\n",
    "#     filters = input_tensor.shape[-1]\n",
    "#     se = layers.GlobalAveragePooling2D()(input_tensor)\n",
    "#     se = layers.Reshape((1, 1, filters))(se)\n",
    "#     se = layers.Conv2D(filters // ratio, 1, activation='relu')(se)\n",
    "#     se = layers.Conv2D(filters, 1, activation='sigmoid')(se)\n",
    "#     return layers.Multiply()([input_tensor, se])\n",
    "\n",
    "# def mb_conv1_block(input_tensor, kernel_size=3, filters=16, dropout_rate=0.2):\n",
    "#     x = layers.Conv2D(filters, (kernel_size, kernel_size), padding='same')(input_tensor)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = se_block(x)\n",
    "#     x = layers.Conv2D(filters, (1, 1), padding='same')(x)\n",
    "#     x = layers.Dropout(dropout_rate)(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     return layers.Add()([x, input_tensor])\n",
    "\n",
    "# def mb_conv6_block(input_tensor, kernel_size=3, filters=16, dropout_rate=0.2):\n",
    "#     input_filters = input_tensor.shape[-1]\n",
    "#     if input_filters != filters:\n",
    "#         shortcut = layers.Conv2D(filters, (1, 1), padding='same')(input_tensor)\n",
    "#         shortcut = layers.BatchNormalization()(shortcut)\n",
    "#     else:\n",
    "#         shortcut = input_tensor\n",
    "\n",
    "#     x = layers.Conv2D(filters * 6, (1, 1), padding='same')(input_tensor)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.DepthwiseConv2D((kernel_size, kernel_size), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = se_block(x)\n",
    "#     x = layers.Conv2D(filters, (1, 1), padding='same')(x)\n",
    "#     x = layers.Dropout(dropout_rate)(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     return layers.Add()([x, shortcut])\n",
    "\n",
    "# def BasicBlock(input_tensor, c_in, c_out):\n",
    "#     x = layers.Conv2D(c_out, (3, 3), padding='same')(input_tensor)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Conv2D(c_out, (3, 3), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     return x\n",
    "\n",
    "# def attention_branch(input_tensor):\n",
    "#     x = BasicBlock(input_tensor, 1, 2)\n",
    "#     x = BasicBlock(x, 2, 4)\n",
    "#     x = BasicBlock(x, 4, 8)\n",
    "#     x = BasicBlock(x, 8, 16)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Conv2D(16, (1, 1), padding='same')(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.Conv2D(1, (1, 1), padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     A = layers.Activation('sigmoid')(x)\n",
    "#     return A\n",
    "\n",
    "# def EfficientNet_A0_with_attention(input_shape=(1, hp.F, hp.T), dropout_rate=0.2):\n",
    "#     inputs = layers.Input(shape=input_shape)\n",
    "#     x = layers.Conv2D(32, (1, 1), padding='same')(inputs)\n",
    "#     x = mb_conv1_block(x, filters=32, dropout_rate=dropout_rate)\n",
    "#     x = mb_conv6_block(x, filters=32, dropout_rate=dropout_rate)\n",
    "#     x = mb_conv6_block(x, filters=48, dropout_rate=dropout_rate)\n",
    "#     x = mb_conv6_block(x, filters=48, dropout_rate=dropout_rate)\n",
    "#     x = mb_conv6_block(x, filters=64, dropout_rate=dropout_rate)\n",
    "#     x = mb_conv6_block(x, filters=64, dropout_rate=dropout_rate)\n",
    "#     x = layers.Conv2D(1280, (1, 1), padding='same')(x)\n",
    "#     x = layers.Dropout(dropout_rate)(x)\n",
    "#     R = layers.GlobalAveragePooling2D()(x)\n",
    "#     R_output = layers.Dense(1, activation='sigmoid')(R)\n",
    "#     A = attention_branch(inputs)\n",
    "#     A_adjusted = layers.GlobalAveragePooling2D()(A)\n",
    "#     A_adjusted = layers.Reshape((1,))(A_adjusted)\n",
    "#     combined_output = R_output + R_output * A_adjusted\n",
    "#     model = models.Model(inputs, combined_output)\n",
    "#     return model\n",
    "\n",
    "# Adding regularization \n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, losses\n",
    "\n",
    "def se_block(input_tensor, ratio=16):\n",
    "    filters = input_tensor.shape[-1]\n",
    "    se = layers.GlobalAveragePooling2D()(input_tensor)\n",
    "    se = layers.Reshape((1, 1, filters))(se)\n",
    "    se = layers.Conv2D(filters // ratio, 1, activation='relu')(se)\n",
    "    se = layers.Conv2D(filters, 1, activation='sigmoid')(se)\n",
    "    return layers.Multiply()([input_tensor, se])\n",
    "\n",
    "def mb_conv1_block(input_tensor, kernel_size=3, filters=16, dropout_rate=0.2):\n",
    "    x = layers.Conv2D(filters, (kernel_size, kernel_size), padding='same')(input_tensor)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = se_block(x)\n",
    "    x = layers.Conv2D(filters, (1, 1), padding='same')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    return layers.Add()([x, input_tensor])\n",
    "\n",
    "def mb_conv6_block(input_tensor, kernel_size=3, filters=16, dropout_rate=0.2):\n",
    "    input_filters = input_tensor.shape[-1]\n",
    "    if input_filters != filters:\n",
    "        shortcut = layers.Conv2D(filters, (1, 1), padding='same')(input_tensor)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "    else:\n",
    "        shortcut = input_tensor\n",
    "\n",
    "    x = layers.Conv2D(filters * 6, (1, 1), padding='same')(input_tensor)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.DepthwiseConv2D((kernel_size, kernel_size), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = se_block(x)\n",
    "    x = layers.Conv2D(filters, (1, 1), padding='same')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    return layers.Add()([x, shortcut])\n",
    "\n",
    "def BasicBlock(input_tensor, c_in, c_out):\n",
    "    x = layers.Conv2D(c_out, (3, 3), padding='same')(input_tensor)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(c_out, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "def attention_branch(input_tensor):\n",
    "    x = BasicBlock(input_tensor, 1, 2)\n",
    "    x = BasicBlock(x, 2, 4)\n",
    "    x = BasicBlock(x, 4, 8)\n",
    "    x = BasicBlock(x, 8, 16)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(16, (1, 1), padding='same')(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(1, (1, 1), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    A = layers.Activation('sigmoid')(x)\n",
    "    return A\n",
    "\n",
    "def TransformerBlock(embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "    inputs = layers.Input(shape=(None, embed_dim))\n",
    "    attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n",
    "    attention_output = layers.Dropout(rate)(attention_output)\n",
    "    out1 = layers.LayerNormalization()(inputs + attention_output)\n",
    "    ffn_output = layers.Dense(ff_dim, activation=\"relu\")(out1)\n",
    "    ffn_output = layers.Dense(embed_dim)(ffn_output)\n",
    "    ffn_output = layers.Dropout(rate)(ffn_output)\n",
    "    final_output = layers.LayerNormalization()(out1 + ffn_output)\n",
    "    return models.Model(inputs, final_output)\n",
    "\n",
    "def EfficientNet_A0_with_attention(input_shape=(1, 257, 400), dropout_rate=0.2):  # Example dimensions\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (1, 1), padding='same')(inputs)\n",
    "    x = mb_conv1_block(x, filters=32, dropout_rate=dropout_rate)\n",
    "    x = mb_conv6_block(x, filters=32, dropout_rate=dropout_rate)\n",
    "    x = mb_conv6_block(x, filters=48, dropout_rate=dropout_rate)\n",
    "    x = mb_conv6_block(x, filters=48, dropout_rate=dropout_rate)\n",
    "    x = mb_conv6_block(x, filters=64, dropout_rate=dropout_rate)\n",
    "    x = mb_conv6_block(x, filters=64, dropout_rate=dropout_rate)\n",
    "    x = layers.Conv2D(1280, (1, 1), padding='same')(x)\n",
    "    x = layers.Reshape((-1, 1280))(x)  # Reshape for Transformer block\n",
    "    transformer_model = TransformerBlock(1280, 8, 512)\n",
    "    x = transformer_model(x)  # Apply the transformer model directly\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    R_output = layers.Dense(1, activation='sigmoid')(x)\n",
    "    A = attention_branch(inputs)\n",
    "    A = layers.GlobalAveragePooling2D()(A)\n",
    "    A_adjusted = layers.Reshape((1,))(A)\n",
    "    combined_output = R_output + R_output * A_adjusted\n",
    "    model = models.Model(inputs, combined_output)\n",
    "    return model\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    bce = losses.binary_crossentropy(y_true, y_pred)\n",
    "    reg = regularizers.l2(0.06)\n",
    "    regularization = tf.reduce_sum(reg(y_pred)) * tf.reduce_max(y_pred)\n",
    "    loss = bce + regularization\n",
    "    return loss\n",
    "\n",
    "# Adjust hp.F and hp.T to your actual feature and time dimensions\n",
    "model = EfficientNet_A0_with_attention(input_shape=(1, hp.F, hp.T))\n",
    "model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "labels\n",
    "\n",
    "one_ctr = 0 \n",
    "zero_ctr = 0 \n",
    "\n",
    "for i in labels : \n",
    "    if i == 0: \n",
    "        zero_ctr +=1\n",
    "    else : \n",
    "        one_ctr +=1\n",
    "\n",
    "print(one_ctr , zero_ctr)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_curve\n",
    "import os, tensorflow as tf, pickle\n",
    "\n",
    "model_path = 'model.keras'\n",
    "test_data_path = 'test_preprocessed.txt'\n",
    "directory_path = 'LA/ASVspoof2019_LA_eval/flac'\n",
    "pickle_path = 'test_data_cache.pkl'\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    reg = tf.keras.regularizers.l2(0.06)\n",
    "    return bce + tf.reduce_sum(reg(y_pred)) * tf.reduce_max(y_pred)\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    model = tf.keras.models.load_model(model_path, custom_objects={'custom_loss': custom_loss})\n",
    "    print(\"Model loaded from disk\")\n",
    "else:\n",
    "    model = EfficientNet_A0_with_attention(input_shape=(1, hp.F, hp.T))\n",
    "    model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
    "    print(\"Model created from scratch\")\n",
    "\n",
    "class ClassWiseAccuracy(Callback):\n",
    "    def __init__(self, train_dataset, val_dataset):\n",
    "        super().__init__()\n",
    "        self.train_dataset, self.val_dataset = train_dataset, val_dataset\n",
    "        self.epoch_counter = 0\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch_counter += 1\n",
    "        if self.epoch_counter % 5 == 0:\n",
    "            for dataset, name in [(self.train_dataset, 'Train'), (self.val_dataset, 'Test')]:\n",
    "                y_true, y_scores = [], []\n",
    "                for x, y in dataset:\n",
    "                    y_true.extend(y.numpy())\n",
    "                    preds = self.model.predict(x, verbose=0)\n",
    "                    preds = np.hstack([1 - preds, preds]) if preds.shape[1] == 1 else preds\n",
    "                    y_scores.extend(softmax(preds, axis=1)[:, 1])\n",
    "                y_true, y_scores = np.array(y_true), np.array(y_scores)\n",
    "                fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "                eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]\n",
    "                y_pred = (y_scores > thresholds[np.nanargmin(np.abs(fpr - (1 - tpr)))]).astype(int)\n",
    "                class_acc = [np.mean(y_pred[y_true == k] == y_true[y_true == k]) for k in np.unique(y_true)]\n",
    "                for i, acc in enumerate(class_acc):\n",
    "                    print(f\"{name} Accuracy for class {i}: {acc}\")\n",
    "                print(f\"{name} EER: {eer}\")\n",
    "\n",
    "if os.path.exists(pickle_path):\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        tensors, labels = pickle.load(f)\n",
    "        \n",
    "else:\n",
    "    file_names, labels = [], []\n",
    "    with open(test_data_path) as text_file:\n",
    "        for line in text_file:\n",
    "            file_name, label = line.split()\n",
    "            file_names.append(file_name)\n",
    "            labels.append(int(label))\n",
    "    tensors = []\n",
    "    for file_name in file_names:\n",
    "        signal, rate = read_flac_file(os.path.join(directory_path, file_name))\n",
    "        lfcc = extract_lfcc(signal, rate)\n",
    "        tensors.append(tf.expand_dims(tf.convert_to_tensor(lfcc), axis=0)[0])\n",
    "    with open(pickle_path, 'wb') as f:\n",
    "        pickle.dump((tensors, labels), f)\n",
    "\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((tensors, labels)).shuffle(len(tensors)).batch(32)\n",
    "train_dataset = dataset\n",
    "model.fit(train_dataset, epochs=hp.n_epochs_training, callbacks=[ClassWiseAccuracy(train_dataset, dataset_val)])\n",
    "model.save(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    acc = history.history['accuracy']\n",
    "    # val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    # val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    plt.plot(epochs, acc, label='Training acc')\n",
    "    # plt.plot(epochs, val_acc, label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, label='Training loss')\n",
    "    # plt.plot(epochs, val_loss, label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(model.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from pkl\n",
    "\n",
    "# model = tf.keras.models.load_model('model.keras')\n",
    "\n",
    "\n",
    "# model.evaluate(dataset_val)\n",
    "# predictions = model.predict(dataset_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
